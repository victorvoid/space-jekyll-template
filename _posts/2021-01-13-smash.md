---
layout: post
title:  "SMASH:One-shot NAS"
image: ''
date:   2021-01-13 22:57
tags:
- 论文翻译
description: ''
categories:
- ML
---


> 摘要：设计深度神经网络的体系结构需要专业知识和大量的计算时间。 我们提出了一种通过学习辅助超网来加速体系结构选择的技术，它生成基于该模型体系结构的主模型的权重。 通过比较网络的相对验证性能与HyperNet生成的权重，我们可以有效地搜索广泛的体系结构，以牺牲一个单一的训练运行。 为了方便这种搜索，我们开发了一种基于内存读写的灵活机制，它允许我们定义广泛的网络连接模式，以ResNet、DenseNet和分形Net块作为特例。 我们在CIFAR-10和CIFAR-100、STL-10、ModelNet10和Imagenet32x32上验证了我们的方法(SMASH)，在类似大小的手动设计网络中获得了竞争性能。

## 1、介绍

深度神经网络的高性能受到广泛的工程和验证成本的调节，为给定的问题找到最适合的结构。 高层次的设计决策，如深度、每层单位和层连通性并不总是显而易见的，诸如Inception[39]、ResNet[13]、 FractalNets[20]和 DenseNets [15] 等模型的成功表明了复杂设计模式的好处。 即使拥有专业的知识，确定哪些设计元素交集在一起需要足够的实验时间。

在本工作中，我们建议绕过完全训练候选模型的昂贵过程，而是训练辅助模型HyperNet[12]，以动态生成具有可变体系结构的主模型的权重。 虽然这些生成的权重比固定体系结构的自由学习的权重差，但我们利用 observation [21]，即不同网络在训练早期的相对性能(即与最终最优的某种距离)通常提供了在最优性下性能的有意义的指示。 通过使用生成的权重比较一组体系结构的验证性能，我们可以以单个培训运行的成本大致对许多体系结构进行排序。

为了方便这种搜索，我们开发了一种基于内存读写的灵活方案，允许我们定义不同范围的体系结构，以 ResNets、 DenseNets,和FractalNets作为特例。 我们验证了我们在CIFAR-10和CIFAR-100[19]、Imagenet32x32[7]、ModelNet10[41]和STL-100 [8]上通过超网进行一次模型架构搜索(SMASH)进行卷积神经网络(CNN)，在类似大小的手工设计的网络中实现了有竞争力的性能。

## 2、相关工作



现代实用的超参数优化方法依赖于随机搜索[4]或贝叶斯优化(BO)[33,34]，将模型性能视为黑匣子。 虽然这些方法是成功的，但需要多次培训才能进行评估（即使从一个良好的初始模型开始），而在BO的情况下，通常不用于指定可变长度的设置，例如正在考虑的模型的连通性和结构。 与此相关的是，基于强盗的方法[21]提供了一个框架，通过采用自适应的早期停止策略来有效地探索超参数空间，并将更多的资源分配给在训练早期显示出希望的模型。

进化技术[10,37,38,40]提供一种灵活的方法，从琐碎的初始条件中发现混合模型，但往往难以扩展到搜索空间巨大的深层神经网络，即使具有巨大的计算能力[26]。

强化学习方法[3,44]已被用来训练代理使用策略梯度生成网络定义。 这些方法从琐碎的体系结构开始，并发现实现非常高性能的模型，但需要12到15000次完整的培训才能找到解决方案。

Saxe等人的方法 [29]是最像我们自己的方法，他建议只训练具有随机卷积权的卷积网络的输出层来有效地探索各种体系结构。虽然比完全训练整个网络端到端更有效，但这种方法似乎并没有扩展到更深的网络[42]。 我们的方法在概念上是相似的，但用通过Hyper Nets[12]生成的权重替换随机权重，这是一类通过使用辅助模型[9,17,27,31]动态调整权重的技术之一。 在我们的例子中，我们学习从架构的二进制编码到权重空间的转换，而不是学习根据模型输入来适应权重。

我们的方法被明确地设计为评估广泛的模型配置（在连接模式和每层单位方面），但不涉及其他超参数，如正则化、学习速率调度、权重初始化或数据增强。与上述进化的方法或RL方法不同，我们探索了一个有点像预定义的设计空间，而不是从一个琐碎的模型开始指定一组可用的网络元素。虽然我们仍然考虑一组丰富的体系结构，但我们的方法不能单独发现完全新的结构，并且受到限制因为它只动态地生成模型参数的特定子集。 此外，虽然我们的方法不是进化的，但我们的编码方案让人想起CGP[23]。

随机正则化技术，如Dropout[35]、Swapout[32]、DropPath或随机深度[14]表面上类似于我们的方法，因为它们通过在固定网络体系结构中随机删除连接路径来获得可变配置。例如，卷积神经结构[30]利用这一思想试图将一个大网络训练为通过丢弃路径产生的所有子网络的隐式集合。 我们的方法与其不同的一个关键元素是，我们网络中每个节点的权重是动态生成的，而不是固定的；如果Dropout集成访问以前没有训练过的单元，则该单元的权重将完全不被调整。 我们的方法甚至推广到以前看不见的体系结构，我们在随机条件下训练的网络只是我们用来评估各种网络配置的代理，而不是最终的模型。

## 3、超网络一热模型架构搜索

在SMASH（算法1）中，我们的目标是根据每个配置的验证性能对一组神经网络配置进行相对排序，我们使用辅助网络生成的权重来完成这些配置。 在每个训练步骤中，我们随机采样一个网络体系结构，使用HyperNet生成该体系结构的权重，并通过反向传播对整个系统进行端到端的训练。 当模型完成训练时，我们使用HyperNet生成的权重对一些随机体系结构进行采样，并在验证集上评估它们的性能。 然后，我们选择具有最佳估计验证性能的体系结构，并正常训练其权重。

~~~python
# Algorithm 1 SMASH
input Space of all candidate architectures, Rc # 所有候选体系结构的输入空间，RC
	Initialize HyperNet weights H # 初始化超网权重H
	loop
		Sample input minibatch xi, random architecture c and architecture weights W = H(c) # 样本输入minibatchxi，随机体系结构c和体系结构权重W=H(C)
        # 获取训练错误ET=FC(W，XI)=FC(H(C)，XI)，反向传播和更新H
		Get training error Et = fc(W, xi) = fc(H(c), xi), backprop and update H
	end loop
	loop
    	# 样本随机c和评估验证集EV=FC(H(C)，XV)上的错误)
		Sample random c and evaluate error on validation set Ev = fc(H(c), xv)
	end loop
 # 固定体系结构，用自由变化的权重W正常训练
Fix architecture and train normally with freely-varying weights W 
~~~

SMASH包括两个核心组件：我们对体系结构进行采样的方法和对给定体系结构进行权重采样的方法。 对于前者，我们开发了前馈网络的内存库视图，允许将复杂的、分支拓扑和编码表示的拓扑作为二进制向量。 对于后者，我们使用HyperNet[12]，它学习直接从二进制架构编码映射到权重空间。

我们假设只要HyperNet学会产生合理的权重，当使用通常训练的权重时，具有生成权重的网络的验证误差将与性能相关，而体系结构的差异是变化的主要因素。在整个论文中，我们将训练的第一部分(HyperNet、可变体系结构主网络和任何自由学习的主网络权重)中的整个设备称为SMASH网络，我们将具有自由学习权重但SMASH派生体系结构的网络称为结果网络。

### 3.1定义可变网络配置

![image-20210113001255444](C:\Users\meng1\AppData\Roaming\Typora\typora-user-images\image-20210113001255444.png)

​														图1：ResNet、DenseNet和FractalNet 的内存库表示

为了探索具有可变深度、连接模式、层大小和更大范围的广泛体系结构，我们需要一种灵活的机制来定义这种体系结构，我们也可以很容易地将其编码为HyperNet的条件向量。 为此，我们支持前馈网络的“内存库”观点。

我们不把网络看作是应用于正向传播信号的一系列操作，而是把网络看作是有一组内存库（最初是充满零的张量），它可以读写。 因此，每个层都是从内存的子集读取数据、修改数据并将结果写入内存的另一个子集的操作。 对于单个分支体系结构，网络有一个大的内存库，它在每个操作中读取和覆盖(或者，对于ResNet，添加。 像DenseNet这样的分支体系结构从所有以前写的读取bank并写入空bank，而分形网遵循更复杂的读写模式，如图1所示。

我们的基本网络结构由多个块组成(图2(B)，其中每个块在给定的空间分辨率下有一组内存库，与大多数CNN架构一样，空间分辨率依次减半。下采样是通过1x1卷积来完成的，然后是平均池[15]，1x1卷积和完全连接的输出层的权重被自由学习，而不是生成。

![image-20210113200211203](C:\Users\meng1\AppData\Roaming\Typora\typora-user-images\image-20210113200211203.png)

​																							图2

在对体系结构进行抽样时，每个块随机抽样bank的数量和每个bank的通道数。 在定义块中的每个层时，我们随机选择读写模式和要对读数据执行的OP的定义。 当从多个bank读取时，我们沿着通道轴连接读取张量，当写入bank时，我们添加到每个bank当前的张量中。 对于之前有报道的实验，我们只在一个block从bank读和写(即。 一个空间分辨率)，虽然人们可以使用调整大小，以允许从任何块读和写，类似于[30]。

每个OP包括1x1卷积（减少传入通道的数量），然后是具有非线性的可变卷积数，如图2(A)所示。 我们随机选择四个卷积中的哪一个是活动的，以及它们的滤波器大小、膨胀因子、组数和输出单元数(即层大小)。 1x1conv的输出通道数是OP输出通道数的“瓶颈比”。

如3.2节所述，1x1卷积的权重由HyperNet生成，而其他卷积通常是学习的参数。 为了确保可变深度，我们为每个块学习了一组4个卷积，并在块内的所有操作中共享它。 我们限制了最大滤波器的大小和输出单元的数量，当采样OP使用小于两者的最大值时，我们只需将权重张量切片到所需的大小。 固定的转换卷积和输出层根据传入的非空内存库的数量使用相同的切片。 有关这一计划的确切细节见附录。

在设计我们的方案时，我们努力最小化静态学习参数的数量，将网络的大部分容量放置在HyperNet中。 这一目标的一个显著结果是，我们只在下样本层和输出层之前使用批处理规范[16]，因为特定于层的运行统计数据很难动态生成。 我们实验了几种不同的归一化方案，包括权重规范[28]、层规范[2]和规范支持[1]但发现它们在训练中是不稳定的。

相反，我们使用了一个简化版本的权重规范，其中我们将每个生成的1x1滤波器的全部除以它的欧几范数（而不是单独规范每个信道），我们发现这对于SMASH很好地工作，并且在固定结构网络中使用时只会导致精度的微小下降。 在OP中没有其他卷积被归一化。

### 3.2 学习将体系结构映射到权重

![image-20210113195626437](C:\Users\meng1\AppData\Roaming\Typora\typora-user-images\image-20210113195626437.png)

​											图3：一个未滚动的图，它的等效内存库表示和编码嵌入。

超网[12]是一种神经网络，用于参数化另一个网络，即主网络的权重。 对于具有参数H的静态HyperNet，主网络权重W是一些函数(例如。 学习嵌入z的多层感知器，使得学习权值的数量通常小于主网络的全部权值。 对于动态超网，权重W是基于网络输入x生成的，对于递归网络，则基于当前输入xt和先前的隐藏状态ht1生成的。超网[12]是一种神经网络，用于参数化另一个网络，即主网络的权重。 对于具有参数H的静态HyperNet，主网络权重W是一些函数(例如。 学习嵌入z的多层感知器，使得学习权值的数量通常小于主网络的全部权值。 对于动态超网，权重W是基于网络输入x生成的，对于递归网络，则基于当前输入xt和先前的隐藏状态ht1生成的。

我们提出了一种动态超网的变体，它基于主网络体系结构c的张量编码来生成权重W。我们的目标是学习一个映射W=H(C)，对于任何给定的c，它相当接近最优W，这样我们就可以使用HyperNet生成的权重根据验证误差对每个c进行排序。 因此，我们采用了一种c的布局方案，以便能够对具有广泛可变拓扑的体系结构进行采样，与标准库中可用的工具箱兼容，并使c的维度尽可能可解释。

我们的HyperNet是完全卷积的，使得输出张量W的维数随输入c的维数而变化，我们制作了标准格式BCHW的4D张量，批处理大小为1，因此没有输出元素是完全独立的。 这允许我们通过增加c的高度或宽度来改变主网络的深度和宽度。在这种方案下，W的空间维数的每一个切片对应于c的特定子集。描述使用W子集的OP的信息嵌入到相应c片的信道维数中。

例如，如果一个OP从内存库1、2和4读取，然后写入2和4，那么c的相应切片的第一、第二和第四通道将填充1s（指示读取模式），该切片的第六和第八通道将填充1s（指示写入模式）。 其余的OP描述以类似的1-热的方式编码在其余的通道中。 我们只根据OP的输出单元数编码到c的宽度范围，因此不对应于W的任何元素的c元素是空的。

这种方案的简单实现可能需要c的大小等于W的大小，或者让HyperNet使用空间上采样来产生更多的元素。 我们发现这些选择作用不佳，相反，我们采用了一种基于信道的权重压缩方案，该方案减小了c的大小，并使HyperNet的表示能力与主网络的表示能力成正比。 我们将c的空间范围定为W的大小的几分之k，并将k个单元放置在HyperNet的输出处，然后将得到的1×k×高度和宽度张量重塑到所需的大小，选择为DN2，其中N是最小内存库大小，D是一个“深度压缩”超参数，表示W的多少片对应于c的一个切片。有关此方案的完整细节（以及编码策略的其余部分）可在附录B中获得。

## 4、实践

我们将SMASH应用于几个数据集，这既是为了与其他技术进行基准测试，也是为了研究SMASH网络的行为。 首先，我们有兴趣确定使用SMASH生成的权重(“SMASH评分”)的网络的验证错误是否与正常训练的网络的验证相关，如果是，则确定相关性存在的条件。 我们还感兴趣的是学习的体系结构到新的数据集和领域的可转移性，以及这与正常（按重量计算）迁移学习的关系。

我们的代码1是用PyTorch[24]编写的，以利用动态图，并根据内存库视图显式地定义每个采样网络，以避免混淆其内部工作原理（可能更有效）抽象。 为了简洁起见，我们省略了许多超参数细节；附录中提供了完整的细节，以及我们最好的体系结构的可视化。

### 4.1测试SMASH相关性

首先，我们在CIFAR-100上训练了一个SMASH网络300个epochs，使用标准的 annealing schedule[15]，然后对250个随机体系结构进行采样，并在由原始训练集中的5000个随机示例组成的持久验证集上评估它们的SMASH评分。 然后，我们根据它们的SMASH评分对体系结构进行排序，并选择每5个体系结构进行完整的训练和评估，使用30个时代的加速训练计划。 

对于这些网络，我们认为是SMASHv1，这个体系结构使用固定的内存库大小（尽管每个块中有可变数量的银行）、OP主体中的单个固定3x3Conv(而不是Convs的可变2x2数组)、单个组和固定瓶颈比为4。 变量元素包括读写模式、输出单元数和3x3滤波器的膨胀因子。 当采样体系结构时，我们将随机的上界计算预算分配给每个块。

在这些条件下，我们观察到SMASH评分与真实验证性能之间的相关性（图4），表明SMASH生成的权重可以用来快速比较体系结构。 不夸大这一说法是至关重要的；这个测试可以说是一个单一的数据点，表明相关性在这种情况下是成立的，但既不保证相关性的通用性，也不意味着它将持有的条件范围。 运行这个实验的费用禁止令人满意的重复试验，所以我们构建了不同的实验。

对于我们的第二个实验，我们训练了一个低成本的SMASH网络（以允许更快速的测试），相对于主网络来说，HyperNet要小得多（尽管仍然是生成的权重与自由学习的权重的标准比率）。 我们预计，能力下降的HyperNet将无法学习为整个体系结构生成良好的权重，并且SMASH评分与真实性能之间的相关性因此是弱的或不存在的。 这项研究的结果如图5(A)所示，在图中我们可以观察到相关关系的细分。

对于我们的第三个实验，我们训练了一个高预算的SMASH网络，并大大增加了通常学习的参数与HyperNet生成的参数的比率，使得网络的大多数模型容量是在非生成权重中。 在这些条件下，用SMASH生成的权重实现的验证误差远低于具有典型比率的等效SMASH网络实现的验证误差，但得到的顶级模型不像性能一样，我们发现（在非常有限的相关测试中，我们执行的）SMASH评分与真实性能无关。这突出了两个潜在的陷阱：首先，如果HyperNet不负责足够的网络容量，那么生成的聚合和学习的权重可能不足以很好地适应每个采样的体系结构，因此远远不能用于比较体系结构。 第二，比较两个单独的SMASH网络的SMASH分数可能会产生误导，因为SMASH分数是正常学习和生成权重的函数，而具有更固定权重的网络即使得到的网络不更好，也可能获得更好的SMASH分数。

### 4.2 结构梯度下降

作为我们方法的附加测试，我们检查HyperNet是否已经学会考虑c中的体系结构定义，或者它是否忽略c，并且天真地生成一个碰巧工作良好的无条件权重子空间。 我们通过采样一个架构来“欺骗”超网，但要求它通过损坏编码张量c（例如）为不同的体系结构生成权重。 通过洗牌膨胀值)。 对于给定的体系结构，我们发现当使用正确的编码张量时，SMASH验证性能始终是最高的，这表明HyperNet确实学会了从体系结构到权重的可传递映射。

在此基础上，我们假设，如果HyperNet学习一个有意义的映射W=H(C)，那么分类误差E=f(W，x)=f(H(C)，x)可以反向传播以找到dEDC，提供相对于体系结构本身的误差的近似度量。 如果这是正确的，那么根据dEDC向量（在我们的方案的约束范围内）扰动体系结构应该允许我们通过类似梯度下降的过程来指导体系结构搜索。 我们用这个想法进行的初步测试没有产生比随机干扰体系结构定义更好的SMASH分数，尽管我们怀疑这部分是由于我们缺乏一个直观满意的离散体系结构空间更新规则。

### 4.3  迁移学习

最初在一个大型数据集上学习的权重模型通常优于在较小数据集上从零开始训练的模型；因此，体系结构可能显示相同的行为。 我们在STL-10[8]上进行测试，这是一个类似于CIFAR数据集的96x96图像的小数据集。 我们比较了从CIFAR-100(在STL-10上从头训练的权重)到STL-10上运行SMASH的最佳架构的性能，以及WRN基线。 对于这些实验，我们使用训练集中的全部5，000幅图像；在下面的部分中，我们还包括使用推荐的10倍训练拆分与WRN基线进行比较。

在这种情况下，我们发现CIFAR-100的最佳架构优于STL-10的最佳架构，分别达到17.54%和20.275%。 作为参考，基线WRN28-10和WRN40-4分别实现了15.43%和16.06%。 这呈现了一个有趣的现象：一方面，人们可能期望在STL-10上发现的体系结构更好地调整到STL-10，因为它是在该数据集上专门学习的。 另一方面，CIFAR-100有更多的培训示例，可能使其成为区分良好体系结构（即良好体系结构）的更好的数据集。 CIFAR-100的准确性更能说明通用性。 在CIFAR-100上发现的更好的体系结构性能似乎有利于后一种假设，这表明体系结构搜索从更大的训练集moreso中受益，而不是领域特异性。

接下来，我们将研究我们最好的CIFAR-100体系结构在3D对象分类基准Model Net10[41]上的性能。 我们使用[5]的设置对模型Net10训练集的体素化实例进行训练，并在模型Net10测试集上报告准确性。 我们的8M参数模型的精度为93.28%，而手工设计的Inception-ResNet[5]的精度为93.61%，在较大的ModelNet40数据集上训练了18M参数。

### 4.3 基准化分析

我们在CIFAR-10和100上运行SMASH，从最初的相关实验中增加了我们的搜索空间，包括可变滤波器大小、可变组和图2所示的可变OP结构，并表示由此产生的网络SMASHv2。 我们在表1中报告了在CIFAR-10和100上具有最高SMASH分数的两个结果网络的最终测试性能。

接下来，我们采取我们最好的SMASHv2架构从CIFAR-100，并培训它在STL-10[8]使用推荐的10倍训练分裂，和图像Net32x32[7]。 我们从表2中我们自己的实验和[7]在3中报告的实验中比较了宽ResNet基线。 注意到WRN40-4在STL-10上的更好的性能，我们还训练了我们最好的体系结构的一个变体，只有一个主卷积和3x3滤波器，以比较地减少参数的数量。

我们的SMASHv2网具有16M的参数，最终测试误差在CIFAR-100上达到20.60%，在CIFAR-10上达到4.03。 这种性能与最先进的手工设计的网络并不完全相同，但与其他采用RL[3,44]或进化方法[26,38]的自动设计方法相比，这是很好的。 我们的网络优于大规模进化[26]尽管需要更少的时间来发现（尽管不是从琐碎的模型开始）和10个数量级的计算。 我们的方法优于Meta QNN[3]但落后于神经结构搜索[44]，尽管这两种方法都需要更多的计算时间，而且与神经结构搜索不同，我们不通过超参数网格搜索对我们发现的体系结构进行后处理。

### 4.4 结论

在本工作中，我们探索了一种技术，通过学习网络参数上的模型，以网络的参数形式为条件，加速体系结构的选择。 我们介绍了一种灵活的方案，用于定义网络连接模式和为高度可变的体系结构生成网络权重。 我们的结果证明了使用辅助模型生成的次优权重的性能与使用完全训练的权重的性能之间的相关性，表明我们可以通过这个代理模型有效地探索结构设计空间。尽管不是最先进的性能，但我们的方法在几个数据集中都具有竞争力。

